# -*- coding: utf-8 -*-
"""NICOLE-TRACYCLOTTEY._SportsPrediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zheYQWsCmyeCJ6JwQaNPZMaJ4OvQmT4A
"""

import pandas as pd
from google.colab import drive
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer, make_column_selector
import numpy as np

#drive.mount('/content/drive')

legacy = pd.read_csv("C:\\Users\\Nicole\\Downloads\\male_players (legacy).csv")
legacy.head()

"""### Cleaning the Data"""

legacy = legacy.drop(columns=['player_id', 'long_name', 'player_url', 'fifa_version', 'fifa_update', 'fifa_update_date'])

legacy = legacy.drop(columns= legacy.loc[:, "goalkeeping_speed":"player_face_url"].columns)

legacy

#dropping columns that have na values that are more than 30%
threshold = 0.3
number_of_nas = legacy.isna().sum()
percentage_of_nas = number_of_nas/len(legacy)
columns_to_drop = percentage_of_nas[percentage_of_nas > threshold].index
legacy.drop(columns=columns_to_drop, inplace=True)

legacy

"""Grouping categorical and numerical columns"""

from pandas.core.arrays import categorical
numeric_cols = legacy.select_dtypes(include=['int64','float64'])
categorical_cols = legacy.select_dtypes(include=['object'])

numeric_cols

categorical_cols

"""Imputing with the mean"""

imp = SimpleImputer(strategy = 'mean')

df_filled = pd.DataFrame(imp.fit_transform(numeric_cols), columns=numeric_cols.columns)
df_filled

categorical_cols.fillna(method='ffill')

"""Encoding categorical variables"""

from sklearn.preprocessing import LabelEncoder

label_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    categorical_cols[col] = le.fit_transform(categorical_cols[col])
    label_encoders[col] = le

categorical_cols

new_data = pd.concat([df_filled, categorical_cols], axis=1)
new_data

"""Separating the independent and dependent variable"""

y = new_data['overall']
X = new_data.drop('overall', axis=1)

"""Feature Extraction"""

X_df =  pd.DataFrame(X)

correlations = X_df.corrwith(y)
correlations   #finding the correlations of the 'overall' dataframe with every other variable

#selecting features with correlations that are higher than 0.4 and less than negative 0.5
selected_features = correlations[(correlations > 0.4) | (correlations < -0.4)]
selected_features

#creating a dataframe for the selected features
X_selected = X[selected_features.index]
X_selected

"""Scaling the independent variable"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_selected)

"""Training with Cross-Validation"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

X_train.shape

"""Cross-Validation for Random Forest Regressor"""

from sklearn.ensemble import RandomForestRegressor
import pickle as pkl

rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)
rf_regressor.fit(X_train, y_train)

y_pred = rf_regressor.predict(X_test)
y_pred

"""Cross-Validation for XGBoost Regressor"""

import xgboost as xgb

xgb_regressor = xgb.XGBRegressor(n_estimators=100, random_state=42, objective='reg:squarederror')
xgb_regressor.fit(X_train, y_train)

pred = xgb_regressor.predict(X_test)

"""Cross-Validation for Gradient Boost regressor"""

from sklearn.ensemble import GradientBoostingRegressor

gb_plus_model = GradientBoostingRegressor(loss='absolute_error', learning_rate=0.1, n_estimators=300, max_depth = 1, max_features = 5)
gb_plus_model.fit(X_train, y_train)

pred_y = gb_plus_model.predict(X_test)

"""Evaluating the models"""

from sklearn.metrics import mean_absolute_error

"""Calculating the MAE of the RandomForestRegressor"""

mae_RF = mean_absolute_error(y_test, y_pred)
print(f"Mean Absolute Error (MAE): {mae_RF}")

"""Calculating the MAE of the XGBoostRegressor"""

mae_XGB = mean_absolute_error(y_test, pred)
print(f"Mean Absolute Error (MAE): {mae_XGB}")

""" Calculating the MAE of the GradientBoostingRegressor"""

mae_GB = mean_absolute_error(y_test, pred_y)
print(f"Mean Absolute Error (MAE): {mae_GB}")

"""Fine Tuning the models then retesting

Fine Tuning for random forest
"""

from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [100, 200, 300, 400, 500],
    'max_depth': [None, 10, 20, 30, 40, 50],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['auto', 'sqrt', 'log2']
}

# Initializing the Random Forest Regressor
rf_regressor = RandomForestRegressor(random_state=42)
grid_search = GridSearchCV(estimator=rf_regressor, param_grid=param_grid,
                           cv=3, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)
grid_search.fit(X_train, y_train)

# Get the best parameters and model
best_params = grid_search.best_params_
best_model = grid_search.best_estimator_
print("Best Parameters:", best_params)

y_pred = best_model.predict(X_test)

"""Fine tuning for xgboost"""

param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7],
    'min_child_weight': [1, 3, 5],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0],
    'gamma': [0, 0.1, 0.2],
    'reg_alpha': [0, 0.01, 0.1],
    'reg_lambda': [1, 1.5, 2]
}

xgb_regressor = xgb.XGBRegressor(objective='reg:squarederror')

grid_search2 = GridSearchCV(estimator=xgb_regressor, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)
grid_search2.fit(X_train, y_train)

best_params2 = grid_search2.best_params_
best_model2 = grid_search2.best_estimator_
print("Best parameters found: ", best_params)

pred = best_model2.predict(X_test)
pred

"""Fine Tuning for Gradient Boost Regressor"""

param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 3, 5],
    'subsample': [0.8, 1.0],
    'max_features': ['auto', 'sqrt', 'log2']
}

gb_plus_model = GradientBoostingRegressor()

grid_search3 = GridSearchCV(estimator=gb_plus_model, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)
grid_search3.fit(X_train, y_train)

best_params3 = grid_search3.best_params_
best_model3 = grid_search3.best_estimator_
print("Best parameters found: ", best_params)

pred_y = best_model3.predict(X_test)
pred_y

"""Preparing the dataset for testing"""

players22 = pd.read_csv("C:\\Users\\Nicole\\Downloads\\players_22-1.csv")
players22.head()

players22 = players22.drop(columns= players22.loc[:, "goalkeeping_speed":"nation_flag_url"].columns)

#dropping all columns with na values more than 30%
threshold_2 = 0.3
number_of_nas2 = players22.isna().sum()
percentage_of_nas2 = number_of_nas2/len(players22)
columns_to_drop2 = percentage_of_nas2[percentage_of_nas2 > threshold_2].index
players22.drop(columns=columns_to_drop2, inplace=True)

from pandas.core.arrays import categorical
numeric_testcols = players22.select_dtypes(include=['int64','float64'])
categorical_testcols = players22.select_dtypes(include=['object'])

numeric_testcols

categorical_testcols = categorical_testcols.drop(columns=['long_name'])
categorical_testcols

#imputing the numeric using KNN imputation
from sklearn.impute import KNNImputer
imputer = KNNImputer(n_neighbors=5)
numeric_testcols = pd.DataFrame(imputer.fit_transform(numeric_testcols), columns=numeric_testcols.columns)
numeric_testcols

#imputing the categorical using forward fill
categorical_testcols.fillna(method='ffill')
categorical_testcols

from sklearn.preprocessing import LabelEncoder

#encoding the categorical columns
label_encoders2 = {}
for col in categorical_testcols:
    le2 = LabelEncoder()
    categorical_testcols[col] = le2.fit_transform(categorical_testcols[col])
    label_encoders2[col] = le2

categorical_testcols

new_df = pd.concat([numeric_testcols, categorical_testcols], axis=1)
new_df

y2 = new_df['overall']
X2 = new_df.drop('overall', axis=1)

#extracting the corresponding features from the legacy dataset
testdata = new_df[selected_features.index]

testdata

#scaling the independent variables
X2_scaled = scaler.fit_transform(testdata)

X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2,
                                       test_size=0.25,
                                       random_state=42)
X2_train.shape, X2_test.shape

"""Testing using the random forest regressor"""

finalpredfor_y = rf_regressor.predict(X2_scaled)
finalpredfor_y

#calculating the mean absolute error
mae_RF2 = mean_absolute_error(y2, finalpredfor_y)
print(f"Mean Absolute Error (MAE): {mae_RF2}")

RF_bestmodel = rf_regressor

import os
os.getcwd()

import pickle as pkl
# saving the model in a pickle file
with open('rf_model.pkl', 'wb') as file:
    pkl.dump(RF_bestmodel, file)

